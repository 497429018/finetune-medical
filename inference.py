# ==============================================================================
# ğŸ¥ å¾®è°ƒå‰åæ¨¡å‹æ•ˆæœå¯¹æ¯”è„šæœ¬
# åŠŸèƒ½:
# - åŒæ—¶åŠ è½½åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒåçš„æ¨¡å‹ã€‚
# - å¯¹é¢„è®¾çš„åŒä¸€ä¸ªé—®é¢˜åˆ—è¡¨ï¼Œåˆ†åˆ«è·å–ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”ã€‚
# - å°†å›ç­”å¹¶æ’æ‰“å°ï¼Œæ–¹ä¾¿ç›´è§‚åœ°å¯¹æ¯”å¾®è°ƒå¸¦æ¥çš„æ•ˆæœæå‡ã€‚
# ==============================================================================

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import warnings

# å¿½ç•¥ä¸€äº›æ— å®³çš„è­¦å‘Š
warnings.filterwarnings("ignore")

# ================================
# 1. é…ç½®å‚æ•° (è¯·ç¡®ä¿è·¯å¾„ä¸æ‚¨çš„è®­ç»ƒé…ç½®ä¸€è‡´)
# ================================
# åŸºç¡€æ¨¡å‹ID (å¿…é¡»ä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´)
BASE_MODEL_ID = "/home/mw/input/models2179"
# LoRAé€‚é…å™¨è·¯å¾„ (finetune_medical.pyä¸­final_model_pathçš„è·¯å¾„)
LORA_ADAPTER_PATH = "/home/mw/input/qwen4b-medical-cot"

# ================================
# 2. å¾…æµ‹è¯•çš„é—®é¢˜åˆ—è¡¨
# ================================
TEST_QUESTIONS = [
    "è¡€çƒ­çš„ä¸´åºŠè¡¨ç°æ˜¯ä»€ä¹ˆ?",
    "å¸•é‡‘æ£®å åŠ ç»¼åˆå¾çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ",
    "åµå·¢ç™Œè‚‰ç˜¤çš„å½±åƒå­¦æ£€æŸ¥æœ‰äº›ä»€ä¹ˆï¼Ÿ",
]

# ================================
# 3. æ¨¡å‹åŠ è½½ä¸ç”Ÿæˆå‡½æ•°
# ================================

def load_models_and_tokenizer():
    """åŠ è½½åŸºç¡€æ¨¡å‹ã€å¾®è°ƒåæ¨¡å‹å’Œåˆ†è¯å™¨"""
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹å’ŒLoRAé€‚é…å™¨ï¼Œæ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
    
    # --- ä½¿ç”¨ç›¸åŒçš„é‡åŒ–é…ç½® ---
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    
    # --- åŠ è½½åŸºç¡€æ¨¡å‹ ---
    print("...æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # --- åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ (åŸºç¡€æ¨¡å‹ + LoRAé€‚é…å™¨) ---
    print("...æ­£åœ¨åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")
    tuned_model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
    
    # ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹éƒ½å¤„äºè¯„ä¼°æ¨¡å¼
    base_model.eval()
    tuned_model.eval()
    
    print("âœ… åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒåæ¨¡å‹å‡å·²åŠ è½½å®Œæˆï¼")
    return base_model, tuned_model, tokenizer


def get_model_response(model, tokenizer, question):
    """è·å–å•ä¸ªæ¨¡å‹å¯¹é—®é¢˜çš„å›ç­”"""
    messages = [{"role": "user", "content": question}]
    input_tensor = tokenizer.apply_chat_template(
        messages, 
        add_generation_prompt=True, 
        return_tensors="pt"
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            input_tensor, 
            max_new_tokens=2048,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )
    
    response = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)
    return response

# ================================
# 4. ä¸»æ‰§è¡Œæµç¨‹
# ================================
def main():
    base_model, tuned_model, tokenizer = load_models_and_tokenizer()

    print("\n" + "="*80)
    print("ğŸš€ å¼€å§‹è¿›è¡Œæ¨¡å‹å›ç­”å¯¹æ¯”æµ‹è¯•...")
    print("="*80)

    for i, question in enumerate(TEST_QUESTIONS):
        print(f"\n\n--- â“ é—®é¢˜ {i+1}: {question} ---\n")
        
        # --- è·å–åŸºç¡€æ¨¡å‹çš„å›ç­” ---
        print("â³ æ­£åœ¨è·å– [åŸºç¡€æ¨¡å‹] çš„å›ç­”...")
        base_response = get_model_response(base_model, tokenizer, question)
        print("\n" + "-"*35 + " [åŸºç¡€æ¨¡å‹å›ç­”] " + "-"*35)
        print(base_response)
        
        # --- è·å–å¾®è°ƒæ¨¡å‹çš„å›ç­” ---
        print("\nâ³ æ­£åœ¨è·å– [å¾®è°ƒåæ¨¡å‹] çš„å›ç­”...")
        tuned_response = get_model_response(tuned_model, tokenizer, question)
        print("\n" + "-"*35 + " [å¾®è°ƒåæ¨¡å‹å›ç­”] " + "-"*35)
        print(tuned_response)
        
        print("\n" + "="*80)
        
    print("\nğŸ‰ æ‰€æœ‰é—®é¢˜å¯¹æ¯”å®Œæˆï¼")

if __name__ == "__main__":
    main()