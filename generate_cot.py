# ==============================================================================
# ğŸ¥ åŒ»ç–—CoTæ•°æ®ç”Ÿæˆè„šæœ¬ (Notebookå±•ç¤ºç‰ˆ)
# åŠŸèƒ½:
# - ä»¥å•çº¿ç¨‹ã€é€æ¡å¤„ç†çš„ç®€å•æ¨¡å¼è¿è¡Œï¼Œæ–¹ä¾¿è°ƒè¯•å’Œè§‚å¯Ÿã€‚
# - é»˜è®¤åªå¤„ç†10æ¡æ•°æ®ç”¨äºå¿«é€ŸéªŒè¯ã€‚
# - è¾“å‡ºè¯¦ç»†ã€åˆ†é˜¶æ®µçš„æ—¥å¿—ï¼Œå®Œç¾å¤ç°æ‚¨æä¾›çš„æ—¥å¿—æ ¼å¼ï¼Œé€‚åˆåœ¨Notebookä¸­å±•ç¤ºã€‚
# - åœ¨ä¸€ä¸ªè„šæœ¬å†…å®Œæˆæ•°æ®ç”Ÿæˆã€åˆ†æã€è½¬æ¢å’Œä¿å­˜çš„å…¨è¿‡ç¨‹ã€‚
# ==============================================================================

import json
import os
import ray
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any

# å¯¼å…¥Hugging Faceå’Œdistilabelç›¸å…³åº“
from datasets import load_dataset, Dataset, DatasetDict
from distilabel.models import OpenAILLM
from distilabel.pipeline import Pipeline
from distilabel.steps import StepResources
from distilabel.steps.tasks import TextGeneration

# ================================
# 1. æ ¸å¿ƒå‡½æ•°å®šä¹‰
# ================================

access_key = "sk-792*********************"

def cleanup_ray():
    """å®‰å…¨åœ°å…³é—­å¹¶æ¸…ç†Rayç¯å¢ƒã€‚"""
    try:
        if ray.is_initialized():
            print("ğŸ§¹ æ­£åœ¨æ¸…ç†Rayç¯å¢ƒ...")
            ray.shutdown()
            print("âœ… Rayç¯å¢ƒå·²æ¸…ç†")
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†Rayç¯å¢ƒæ—¶å‡ºç°é”™è¯¯: {e}")

def load_local_dataset(data_path: str, file_pattern: str) -> Optional[Dataset]:
    """ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†ã€‚"""
    print("ğŸ“– åŠ è½½æ•°æ®é›†...")
    path = Path(data_path)
    if not path.exists():
        print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {path}")
        return None

    if path.is_dir():
        print(f"ğŸ” åœ¨ç›®å½• {path} ä¸­æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼ '{file_pattern}' çš„æ–‡ä»¶...")
        files = list(path.glob(f"{file_pattern}.json")) + list(path.glob(f"{file_pattern}.jsonl"))
        if not files:
            print(f"âŒ åœ¨ç›®å½• {path} ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¨¡å¼ '{file_pattern}' çš„JSONæ–‡ä»¶ã€‚")
            return None
        
        print(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªJSONæ–‡ä»¶:")
        for f in files:
            print(f"  - {f.name}")
        file_paths = [str(f) for f in files]
        return load_dataset("json", data_files=file_paths, split="train")
    else:
        return load_dataset("json", data_files=str(path), split="train")

def build_distilabel_pipeline(config: dict) -> Pipeline:
    """æ„å»ºDistilabelç”Ÿæˆç®¡é“ã€‚"""
    print("\nğŸ”§ æ„å»ºç”Ÿæˆç®¡é“...")
    generation_kwargs = {
        "max_new_tokens": config['max_new_tokens'],
        "temperature": config['temperature'],
        "top_p": config['top_p'],
    }
    
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {config['model']}")
    if "reasoner" in config['model'].lower():
        print("ğŸ§  DeepSeek Reasoneræ¨¡å‹ï¼Œæ¨ç†åŠŸèƒ½å°†è‡ªåŠ¨å¯ç”¨")
    print(f"ğŸ“ ä½¿ç”¨æ¨¡æ¿: {config['prompt_template']}")
    print(f"âš™ï¸ ç”Ÿæˆå‚æ•°: {generation_kwargs}")

    with Pipeline(name="medical-cot-generation-simple").ray() as pipeline:
        llm = OpenAILLM(
            base_url="https://api.deepseek.com/v1",
            api_key=access_key,
            model=config['model'],
            generation_kwargs=generation_kwargs,
        )
        TextGeneration(
            name="text_generation",
            llm=llm,
            template=config['prompt_template'],
            input_batch_size=1, # é€æ¡å¤„ç†
            resources=StepResources(replicas=1), # å•è¿›ç¨‹
        )
    print("âœ… ç®¡é“æ„å»ºæˆåŠŸ!")
    return pipeline

def run_generation_pipeline(config: dict) -> Optional[DatasetDict]:
    """è¿è¡Œå®Œæ•´çš„ç”Ÿæˆç®¡é“ã€‚"""
    cleanup_ray()
    
    print("\nğŸš€ å¼€å§‹è¿è¡ŒåŒ»ç–—CoTæ•°æ®ç”Ÿæˆç®¡é“")
    print("="*50)
    print("ğŸ“‹ é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"  æ¨¡å‹: {config['model']}")
    print("="*50)

    dataset = load_local_dataset(config['data_path'], config['file_pattern'])
    if dataset is None:
        return None
        
    print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ! åŒ…å« {len(dataset)} æ¡è®°å½•")
    print(f"ğŸ“Š æ•°æ®é›†å­—æ®µ: {dataset.column_names}")
    
    print("\nğŸ“‹ æ•°æ®æ ·ä¾‹:")
    for i, example in enumerate(dataset.select(range(min(3, len(dataset))))):
        print(f"  æ ·ä¾‹ {i+1}: {str(example['instruction'])[:30]}...")
    
    # é‡‡æ ·10æ¡æ•°æ®è¿›è¡Œæµ‹è¯•
    num_to_process=3
    if len(dataset) > num_to_process:
        print(f"\nâš ï¸ æ³¨æ„ï¼šå°†åªå¤„ç†å‰3æ¡æ•°æ®ç”¨äºç®€å•æµ‹è¯•ã€‚")
        dataset = dataset.select(range(num_to_process))

    pipeline = build_distilabel_pipeline(config)
    
    print("\nğŸ¯ å¼€å§‹ç”Ÿæˆè¿‡ç¨‹...")
    print("â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    distiset = pipeline.run(dataset=dataset, use_cache=False)
    
    print("ğŸ‰ ç”Ÿæˆå®Œæˆ!")
    return distiset

def analyze_and_convert_results(distiset: Optional[DatasetDict], output_dir: str):
    """åˆ†æç”Ÿæˆç»“æœå¹¶è½¬æ¢ä¸ºAlpacaæ ¼å¼ï¼Œæ‰“å°è¯¦ç»†æ—¥å¿—ã€‚"""
    print("\n" + "="*50)
    print("ğŸ“Š ç¬¬äºŒæ­¥: åˆ†æå’Œè½¬æ¢ç»“æœ")
    print("="*50)
    
    if distiset is None:
        print("âŒ æ²¡æœ‰å¯åˆ†æçš„ç»“æœ")
        return None

    try:
        train_data = distiset['default']['train']
        print(f"âœ… æˆåŠŸæå–è®­ç»ƒæ•°æ®")
        print(f"ğŸ“ˆ æ•°æ®æ¡æ•°: {len(train_data)}")
        print(f"ğŸ·ï¸ å­—æ®µåç§°: {train_data.column_names}")
    except (KeyError, TypeError) as e:
        print(f"âŒ æå–è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
        return None

    print("\nğŸ“‹ è¯¦ç»†æ•°æ®åˆ†æ:")
    cot_keywords = ['æ€è€ƒ', 'åˆ†æ', 'é¦–å…ˆ', 'å…¶æ¬¡', 'å› æ­¤', 'æ¨ç†', 'æ­¥éª¤', 'æ ¹æ®', 'åŸºäº']
    
    for i, item in enumerate(train_data):
        print(f"\n--- ç¬¬ {i+1} æ¡æ•°æ® ---")
        print(f"ğŸ“ æŒ‡ä»¤: {item['instruction']}")
        print(f"ğŸ“¥ è¾“å…¥: {item.get('input', '(ç©º)')}")
        print(f"ğŸ“¤ åŸå§‹è¾“å‡º: {str(item['output'])[:100]}...")
        print(f"ğŸ¤– ç”Ÿæˆå†…å®¹: {str(item['generation'])[:150]}...")
        print(f"ğŸ·ï¸ æ¨¡å‹: {item['model_name']}")
        has_cot = any(keyword in item['generation'] for keyword in cot_keywords)
        print(f"ğŸ§  åŒ…å«æ¨ç†è¿‡ç¨‹: {'âœ… æ˜¯' if has_cot else 'âŒ å¦'}")
        print(f"ğŸ“ ç”Ÿæˆé•¿åº¦: {len(item['generation'])} å­—ç¬¦")
        
    print("\nğŸ”„ è½¬æ¢ä¸ºAlpacaæ ¼å¼...")
    alpaca_data = [
        {"instruction": item["instruction"], "input": item.get("input", ""), "output": item["generation"]}
        for item in train_data
    ]
    
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    timestamp = "v1"
    alpaca_file = Path(output_dir) / f"medical_cot_alpaca-{timestamp}.json"
    
    with open(alpaca_file, "w", encoding="utf-8") as f:
        json.dump(alpaca_data, f, ensure_ascii=False, indent=2)
    
    print("âœ… æ•°æ®ä¿å­˜å®Œæˆ!")
    print(f"ğŸ“ Alpacaæ ¼å¼: {alpaca_file}")

    print("\nğŸ“Š è´¨é‡ç»Ÿè®¡:")
    total_count = len(alpaca_data)
    cot_count = sum(1 for item in alpaca_data if any(keyword in item['output'] for keyword in cot_keywords))
    avg_length = sum(len(item['output']) for item in alpaca_data) / total_count if total_count > 0 else 0
    
    print(f"ğŸ“ˆ æ€»æ•°æ®æ¡æ•°: {total_count}")
    print(f"ğŸ§  åŒ…å«æ¨ç†çš„æ•°æ®: {cot_count}")
    print(f"ğŸ“Š æ¨ç†è¦†ç›–ç‡: {cot_count/total_count*100:.1f}%")
    print(f"ğŸ“ å¹³å‡ç”Ÿæˆé•¿åº¦: {avg_length:.0f} å­—ç¬¦")

    print("\nğŸ“‹ Alpacaæ ¼å¼æ ·ä¾‹:")
    for i, item in enumerate(alpaca_data[:2]):
        print(f"\næ ·ä¾‹ {i+1}:")
        print(f"instruction: {item['instruction']}")
        print(f"input: {item['input']}")
        print(f"output: {item['output'][:300]}...")
        print("-" * 40)
        
    return alpaca_data

# ================================
# 2. ä¸»æ‰§è¡Œæµç¨‹
# ================================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("æœ¬é¡¹ç›®æ¥æºäºå’Œé²¸ç¤¾åŒºï¼Œä½¿ç”¨è½¬è½½éœ€è¦æ ‡æ³¨æ¥æº")
    print("\nğŸ¥ åŒ»ç–—CoTæ•°æ®ç”Ÿæˆç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    # if not os.getenv("DEEPSEEK_API_KEY"):
    #     print("âŒ é”™è¯¯: è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ 'DEEPSEEK_API_KEY'")
    #     return
    #access_key = os.getenv("DEEPSEEK_API_KEY")
    #access_key = "sk-792eddb3cdd9480cbe8d0a420c632261"


    config = {
        'model': 'deepseek-reasoner',
        'data_path': 'raw',
        'file_pattern': 'train*',
        'prompt_template': '{{ instruction }}',
        'temperature': 0.1,
        'top_p': 0.8,
        'max_new_tokens': 4096,
        'output_dir': 'output'
    }
    
    print("âš™ï¸ å½“å‰é…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print("=" * 60)
    
    # --- æ­¥éª¤1: è¿è¡Œç”Ÿæˆç®¡é“ ---
    print("\nğŸš€ ç¬¬ä¸€æ­¥: è¿è¡Œç”Ÿæˆç®¡é“")
    distiset_result = run_generation_pipeline(config)
    
    # --- æ­¥éª¤2: åˆ†æå’Œè½¬æ¢ç»“æœ ---
    if distiset_result:
        analyze_and_convert_results(distiset_result, config['output_dir'])
    else:
        print("âŒ ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚")

    # --- æ­¥éª¤3: æœ€ç»ˆæ€»ç»“ ---
    print("\n" + "="*60)
    print("ğŸ‰ ä»»åŠ¡æµç¨‹æ‰§è¡Œå®Œæ¯•!")
    print("="*60)
    
    cleanup_ray()

if __name__ == "__main__":
    main()